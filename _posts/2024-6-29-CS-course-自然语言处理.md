---
layout: post
title: CS-course CS ???? 自然语言处理
tags: mathjax
math: true
date: 2024-6-29 21:00 +0800
---
# 目录
1. [n-gram 模型 统计语言模型](#n-gram-模型-统计语言模型)
2. [词性标注以及隐马尔可夫模型](#词性标注以及隐马尔可夫模型)
3. [中文分词与CRF](#中文分词与crf)
4. [语法解析和句法分析](#语法解析和句法分析)
5. [词向量](#词向量)
   - [CBOW](#cbow)
   - [Skip-gram](#skip-gram)
6. [神经网络语言模型](#神经网络语言模型)
7. [文本分类](#文本分类)
8. [序列标注](#序列标注)
9. [机器翻译](#机器翻译)
10. [预训练语言模型](#预训练语言模型)
11. [大语言模型](#大语言模型)


## n-gram 模型 统计语言模型

## 英文分词
BPE


## 词性标注以及隐马尔可夫模型
HMM的几个重要算法
- Viterbi算法
- 前向算法
- 后向算法
- 最大似然参数估计（这个可以直接推导）（为什么ppt里面写这个是有监督学习
- 无监督学习：前向后向算法

前向算法（Forward Algorithm）、后向算法（Backward Algorithm）和Viterbi算法都是用于隐马尔可夫模型（Hidden Markov Model, HMM）中的基本算法。虽然它们都处理观测序列和隐藏状态序列，但它们的目标和应用有所不同。

### 前向算法与后向算法

**前向算法（Forward Algorithm）**：
- **目标**：计算给定HMM模型和观测序列的情况下，观测序列的概率 \( P(O) \)。
- **用途**：通常用于HMM的参数估计（如Baum-Welch算法）和计算观测序列的似然度。
- **方法**：通过递归计算每个时刻的累积概率，从起始状态到当前时刻的所有可能路径的概率和。

**后向算法（Backward Algorithm）**：
- **目标**：计算从当前时刻到观测序列结束的概率 \( P(O_{t+1:T} | s_t) \)。
- **用途**：与前向算法一起用于HMM的参数估计，以及计算某个状态下未来观测序列的概率。
- **方法**：通过递归计算每个时刻的累积概率，从当前时刻到观测序列结束的所有可能路径的概率和。

前向和后向算法的主要作用是计算观测序列的概率以及用于参数估计。它们的计算复杂度都是 \( O(N^2T) \)，其中 \( N \) 是隐藏状态的数量，\( T \) 是观测序列的长度。

### Viterbi算法

**Viterbi算法**：
- **目标**：找到给定HMM模型和观测序列的情况下，最可能的隐藏状态序列 \( S^* \)。
- **用途**：用于预测最可能的状态序列，常见于语音识别、词性标注和其他需要找出最优路径的应用中。
- **方法**：通过动态规划，递归地计算每个时刻的最大概率路径，并记录路径信息，以便在最后回溯得到最优的隐藏状态序列。

Viterbi算法的主要作用是解码，即找到最可能的隐藏状态序列。它的计算复杂度与前向和后向算法相同，都是 \( O(N^2T) \)。

### 总结

1. **前向算法和后向算法**：
   - 用于计算观测序列的概率。
   - 主要用于HMM的参数估计（如Baum-Welch算法）。
   - 通过累积计算所有可能路径的概率和。

2. **Viterbi算法**：
   - 用于找到最可能的隐藏状态序列。
   - 主要用于解码，即确定最优路径。
   - 通过动态规划计算最优路径的最大概率。

虽然这三种算法都应用于隐马尔可夫模型，但它们解决的问题不同：前向和后向算法用于概率计算和参数估计，而Viterbi算法用于寻找最优状态序列。
Viterbi是个DP算法。

## 中文分词与CRF
CRF的几个重要算法
- Viterbi算法
- 前向算法
- 最大似然参数估计

## 语法解析和句法分析

## 词向量
1. CBOW
CBOW（Continuous Bag of Words，连续词袋模型）是词嵌入（word embedding）技术中的一种，用于将词语表示为连续向量，从而在高维空间中捕捉词语之间的语义关系。CBOW模型是Word2Vec的一部分，由Tomas Mikolov等人在2013年提出。

CBOW模型的核心思想是通过上下文词预测目标词，具体来说，它根据上下文中的词（即目标词周围的词）来预测目标词。这种方法利用了上下文信息，使得嵌入向量能够捕捉词语的语义和语法特征。

以下是CBOW模型的主要概念和工作原理：

1. **输入和输出**：
   - **输入**：上下文词（context words），即目标词周围的词。例如，对于句子"the quick brown fox jumps over the lazy dog"，如果目标词是"fox"，则其上下文词可能是["the", "quick", "brown", "jumps", "over"]。
   - **输出**：目标词（target word），即当前关注的词。

2. **模型结构**：
   - CBOW模型是一个简单的神经网络，通常包括输入层、隐藏层和输出层。
   - **输入层**：上下文词的one-hot向量表示。
   - **隐藏层**：通常是一个线性层，将输入的one-hot向量映射到一个较低维的嵌入空间。
   - **输出层**：输出一个与词汇表大小相同的向量，表示预测目标词的概率分布。

3. **训练过程**：
   - 给定一个上下文窗口（context window），例如窗口大小为2，则每个目标词的上下文词为其前后各两个词。
   - 将上下文词的one-hot向量输入模型，计算它们的嵌入表示，然后将这些嵌入向量求和（或求平均），得到一个固定大小的上下文表示。
   - 通过隐藏层的计算，得到一个预测目标词的概率分布。
   - 使用交叉熵损失函数计算预测词和实际目标词之间的损失，并通过反向传播算法更新模型参数。

4. **优缺点**：
   - **优点**：CBOW模型训练速度快，适合处理大规模语料。它能够有效地学习词语的语义表示，使得词嵌入向量能够捕捉词语之间的语义关系。
   - **缺点**：CBOW模型忽略了词序信息，因为它只考虑上下文词的集合（bag of words）而不考虑它们的顺序。此外，对于频繁词和稀有词的处理效果可能存在差异。

5. **应用**：
   - CBOW模型生成的词嵌入向量可以应用于多种自然语言处理任务，如文本分类、情感分析、机器翻译和问答系统等。
   - 词嵌入向量可以用于计算词语之间的相似度，从而在语义分析和信息检索中发挥重要作用。

总之，CBOW模型通过上下文词预测目标词，生成高维空间中的词嵌入向量，有效地捕捉了词语之间的语义关系，是词嵌入技术中的一种重要方法。


在训练得到CBOW模型后，某个词的嵌入向量可以从模型的权重中提取出来。具体来说，CBOW模型的训练过程会生成一个词汇表中每个词的嵌入向量，这些嵌入向量存储在模型的权重矩阵中。

以下是提取某个词嵌入向量的具体步骤：

1. **构建词汇表**：
   在训练CBOW模型时，会根据训练语料构建一个词汇表（vocabulary），词汇表中每个词都会被分配一个唯一的索引。

2. **嵌入层权重矩阵**：
   CBOW模型的输入层到隐藏层之间通常是一个嵌入层（embedding layer），该层的权重矩阵（embedding matrix）用于将one-hot向量映射到低维嵌入空间。这个权重矩阵的大小为 \(V \times N\)，其中 \(V\) 是词汇表的大小， \(N\) 是嵌入向量的维度。

3. **提取嵌入向量**：
   训练完成后，嵌入层的权重矩阵就包含了所有词的嵌入向量。要提取某个词的嵌入向量，可以按照以下步骤进行：

   - 找到目标词在词汇表中的索引。
   - 使用这个索引从嵌入层的权重矩阵中提取对应的嵌入向量。

以下是用Python和PyTorch实现这一过程的示例：

```python
import torch
import torch.nn as nn
import torch.optim as optim

# 假设我们已经有一个训练好的CBOW模型，这里是一个简单的定义示例
class CBOWModel(nn.Module):
    def __init__(self, vocab_size, embed_size):
        super(CBOWModel, self).__init__()
        self.embeddings = nn.Embedding(vocab_size, embed_size)
        self.linear1 = nn.Linear(embed_size, vocab_size)

    def forward(self, inputs):
        embeds = self.embeddings(inputs).mean(dim=0)
        out = self.linear1(embeds)
        return out

# 假设词汇表和嵌入维度已经定义
vocab_size = 10000  # 词汇表大小
embed_size = 300    # 嵌入向量维度

# 初始化模型
model = CBOWModel(vocab_size, embed_size)

# 训练模型（此处省略训练代码）

# 定义词汇表
word_to_index = {'example': 0, 'word': 1, 'embedding': 2}  # 词汇表示例
index_to_word = {v: k for k, v in word_to_index.items()}

# 提取某个词的嵌入向量
word = 'example'
word_index = word_to_index[word]

# 从嵌入层权重矩阵中提取该词的嵌入向量
word_embedding = model.embeddings.weight[word_index].detach().numpy()

print(f"嵌入向量 for '{word}': {word_embedding}")
```

在这个示例中，假设你已经训练了一个CBOW模型，并且有一个词汇表`word_to_index`，其中包含词汇和其对应的索引。通过访问模型的嵌入层权重矩阵（`model.embeddings.weight`），并使用目标词的索引，可以提取该词的嵌入向量。

这个过程可以应用于任意已训练好的CBOW模型，通过访问嵌入层权重矩阵和词汇表，可以方便地提取任何词的嵌入向量。

Skip-gram

## 神经网络语言模型
RNN/LSTM/GRU

## 文本分类

## 序列标注

## 机器翻译

## 预训练语言模型
Bert ...

## 大语言模型

